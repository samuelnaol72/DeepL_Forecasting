dataset:
  path: "example/dataset/2025_AllSites_Orig.parquet"


preprocessing:
  lags: 24                   # lags window
  horizon: 8                 # future time window
  ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
         # List of sites to preprocess in the dataset
  features: 1                # Number of features per timestep
  channels: 1                # Number of channels in the target output

dataloader:
  batch_size: 32             # Batch size for DataLoader
  shuffle: true              # Shuffle training data

model:
  name: lstm                 # Model type: lstm, gru, lstm41, cnnlstm, etc.
  input_size: 1              # Number of input features (matches channels)
  hidden_size: 64            # Number of hidden units in the model
  num_layers: 2              # Number of layers in the model
  output_size: 8
  dropout: 0.2               # Dropout rate
  save_path: "example/results/models/lstm.pth" # Path to save the trained model

device:
  type: cpu                 # Device to run the tensors on ('cpu' or 'cuda')

training:
  learning_rate: 0.001       # Learning rate for the optimizer
  epochs: 5                 # Number of training epochs
  scheduler_gamma: 0.1       # Factor to reduce the learning rate
  scheduler_patience: 5      # Number of epochs to wait before reducing LR
